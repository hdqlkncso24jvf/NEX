# Overview

This repository is the official code of "**Global and Local Explanations for Negative GNN Predictions**".

* The **full version** of the paper can be accessed at this file: 
  * [`paper_full_version.pdf`](链接地址)
  * 12
  * 2

* The **model checkpoints and dataset** are available at this link: https://drive.google.com/drive/folders/1Cu9WXhRAq-8J4ZBd2REHG9fep4mzLTg8?usp=drive_link.

## Software requirements

```shell
python >= 3.10
torch == 2.3.0
transformers == 4.41.2
datasets >= 2.16.0
accelerate >= 0.30.1
peft >= 0.11.1
trl >= 0.8.6
vllm == 0.4.3
CUDA >= 11.6
flash-attn >= 2.3.0
torch_geometric == 2.5.3 # this is for GNN
numpy >= 1.24.2
scikit-learn >= 1.2.2
scipy >= 1.10.1
torch >= 1.13.1
tqdm >= 4.65.0
```

## Base model and Hardware Requirements

The recommended and default base model is Qwen2.5-1.5B; however, models from the llama, Llama and ChatGLM series, and others are also supported. If you wish to switch the base model, please modify the template as follows:

- **Llama Series**
  - download link: https://huggingface.co/meta-llama
  - Template: `llama2/llama3`

- **Qwen Series**
  - download link: https://huggingface.co/Qwen
  - Template: `qwen`

- **GLM  Series**
  - download link: https://huggingface.co/THUDM
  - Template: `glm3/glm4`


For fine-tuning large language models ranging from 1.5B to 9B parameters, a minimum of one NVIDIA 3090 GPU is required, with a recommended GPU Memory of at least 24GB per card.

For fine-tuning models larger than 13B parameters, at least one V100 GPU is necessary, with a recommended GPU Memory of at least 32GB per card.

## Run

### Language Model Deploy

```shell
python main.py train train.yaml
```

```yaml
# train.yaml
### model
model_name_or_path: your_model_path
quantization_bit: 4 # quantization bits, reduce model size and speed up inference, comment out to use LoRA fine-tuning.

### method
stage: sft # Supervised fine-tuning stage
do_train: true # Flag to indicate training mode
finetuning_type: qlora # Fine-tuning method, can be qlora (quantized LoRA) or other types
lora_target: all # Apply LoRA fine-tuning to all model layers

### dataset
dataset: your_data_set_name
template: mistral # Template for dataset processing, depending on the base model
cutoff_len: 1024 # Maximum sequence length for input data
max_samples: 20 # Maximum number of samples to use from the dataset
overwrite_cache: true # Overwrite cached dataset files if they exist
preprocessing_num_workers: 16 # Number of worker threads for data preprocessing

### output
output_dir: checkpoint_output_path
logging_steps: 10 # Number of steps between logging metrics
save_steps: 500 # Number of steps between saving checkpoints
plot_loss: true # Flag to plot loss during training
overwrite_output_dir: true # Overwrite the contents of the output directory if it exists

### train
per_device_train_batch_size: 1 # Batch size per device during training
gradient_accumulation_steps: 8 # Number of steps to accumulate gradients before updating
learning_rate: 1.0e-4 # Learning rate for the optimizer
num_train_epochs: 3.0 # Total number of training epochs
lr_scheduler_type: cosine # Type of learning rate scheduler (cosine annealing)
warmup_ratio: 0.1 # Proportion of training steps to perform learning rate warmup
fp16: true # Use 16-bit (half precision) floating point arithmetic for training
ddp_timeout: 180000000 # Timeout for distributed data parallel (DDP) training in seconds

### eval
val_size: 0.1 # Proportion of the dataset to use for validation
per_device_eval_batch_size: 1 # Batch size per device during evaluation
eval_strategy: steps # Evaluation strategy to use (evaluate every few steps)
eval_steps: 500 # Number of steps between evaluations
```

### Merge lora checkpoint to get fine-tuned LLMs

```shell
python main.py export merge.yaml
```

```yaml
# merge.yaml
### model
model_name_or_path: your_model_path
adapter_name_or_path: checkpoint_output_path
template: mistral
finetuning_type: qlora # lora

### export
export_dir: merged_model_path
export_size: 2 # model size(GB) of per fragment
export_device: cpu
export_legacy_format: false

```

### Inference

```shell
python -m infer.py model_path dataset_path dataset_name gpu_nums
```

Once the inference of the large language model is completed, you will receive a JSON file formatted as follows, where "category" can serve as a pseudo-label generated by the LLM, and "keywords" can be used for sentence-BERT (such as [SimCSE](https://github.com/princeton-nlp/SimCSE)) to generate the initial embeddings that the GNN initially accepts.

For the convenience of other developers, this project has already pre-completed the inference process and the feature generation process of the LLM. In the `dataset_name_filled.json` file under the LLM folder of each dataset, there are pseudo-labels for rough classification and keywords for feature generation, which have been inferred by the fine-tuned LLM. In the GNN folder of each dataset, the `feature_{keywords_num}.pth` contains the initial embeddings using SimCSE.

# LMiner

The `LMiner` folder contains code for run miner algorithm in level-wise mode.

## Installing dependencies on Ubuntu

GCC version: 7.4.0 or above, support of c++17 standard required.

Install mpi:

```shell
sudo apt-get install openmpi-bin openmpi-doc libopenmpi-dev
```

Install glog:

```shell
sudo apt-get install libgoogle-glog-dev
```

Install gflags:

```shell
sudo apt-get install libgflags-dev
```

Install yaml:

```shell
sudo apt-get install libyaml-cpp-dev
```

## Compile

```shell
mkdir build && cd ./build
cmake ../
make all -j
```

## Run

We use libgrape-lite for multi-process parallelism and openmp for multi-thread parallelism.

For LR discovery, to run with single machine, occupying all threads:

```shell
./build/gar_discover --yaml_file ${yaml_file_name}
```

To run with single machine, occupying a specified number of threads:

```shell
mpirun -n 1 -map-by slot:pe=core_num ./build/gar_discover --yaml_file ${yaml_file_name}
```

To run with multiple machines:

```shell
mpirun -N xxx -n yyy -c zzz ./build/gar_discover --yaml_file ${yaml_file_name}
```

For rule match, to run with single machine, occupying all threads:

```shell
./build/rule_match --yaml_file ${yaml_file_name}
```

The others are same as discovery.

The main loop of raw level-wise discovery can be found in folder LMiner/src/apps/rule_discover/, and the main loop of LR match, pattern matching can be found in folder LMiner/src/apps/rule_match/.

The ER folder contains code for computing 1-WL, where you can employ any feature (such as SimCSE or GloVe) embedding to determine whether a pair of points, as well as all pairs within a graph. Detailed running examples can be viewed within this folder. 

If you want to run LR discovery algorithm, you may need to fill a yaml file in this format:

```yaml
DataGraphPath: # the path for the data graphs
  - VFile: the vertex file for the first data graph
    EFile: the edge file for the first data graph
    MlLiteralEdgesFile: (optional) the edges that are added by the well-trained ml model for the first data graph
  ...
ExpandRound: number of expand round, i.e. total edges to be added
J: depth of the literal tree for horizontal spawning.
SupportBound: the support bound for the gar to be discovered
OutputGarDir: the directory for the discovered gar to export
TimeLimit: time limit for evaluating the support bound of each gar or graph pattern
TimeLimitPerSupp: time limit for it to complete the match of the entire pattern of gar at each support
ConstantFreqBound: the frequency bound for the constant, only the value appear larger than this frequence would be considered
PatternVertexLimit: the limit of pattern vertex
DiameterLimit: the limit of the diameter of the graph pattern
LiteralTypes: # the literal types to be considered
  - constant_literal
  - variable_literal
  - edge_literal
Restrictions: # the restrictions for the gar
  - variable_literal_only_between_connected_vertexes
  - edge_literal_only_between_2_hop_connected_vertexes
  - literals_connected
  - pattern_without_loop
SpecifiedRhsLiteralSet:
  - Type: variable_literal
    XLabel: label of x
    YLabel: label of y
    XAttrKey: attr of x
    YAttrKey: attr of y
TimeLogFile: the path for the time log file
```

An example yaml file for LR discovery may like this:

```yaml
DataGraphPath:
  VFile : dataset/v.csv
  EFile : dataset/e.csv
ExpandRound: 15
J: 3
LiteralTypes:
  - constant_literal
  - variable_literal
  - edge_literal

SupportBound: 1
ConfidenceBound: 0.4

Rule:
  Type: gcr
  PathNumLimit: 3
  PathLengthLimit: 5
  
SpecifiedRhsLiteralSet:
  - Type: variable_literal
    XLabel: 3
    YLabel: 3
    XAttrKey: year
    YAttrKey: year
  
TimeLogFile:  dataset/lr.log
OutputGarDir: dataset/lr

TimeLimit: 3000
TimeLimitPerSupp: 0.5
ConstantFreqBound: 0.09
```

If you want to run LR pattern matching algorithm, you may need to fill a yaml file in this format:

```yaml
DataGraphPath: 
  VFile : vertex file of the data graph
  EFile : edge file of the data graph
  
PatternPath:
  VFile : vertex file of the pattern
  EFile : edge file of the pattern
  XFile : X (lhs) literal file of the pattern
  YFile : Y (rhs) literal file of the pattern
  PivotId : (optional) specify the pivot vertex id, needs to be contained in the Y literals of the pattern

TimeLogFile: time log file
```

# RxGNNs

**Quick Start Guide**

The RxGNNs framework provides two main components for explainable GNN predictions:

```python
from miner import RuleDiscovery, PatternComposer, PredicateSelector

# Initialize the rule discovery with basic configuration
rule_miner = RuleDiscovery(
    data_graph,                      # Your input graph
    motifs=None,                     # Optional pre-defined motifs (auto-generated if None)
    support_threshold=5,             # Minimum support for rules
    confidence_threshold=0.5,        # Minimum confidence for rules
    max_verification_time=50,        # Time limit for rule verification
    max_pattern_combinations=3,      # Limit for pattern composition
    sample_ratio=0.1                 # Sampling ratio for large graphs
)

# Discover explanatory rules
discovered_rules = rule_miner.discover(num_threads=4)

# Access discovered rules
for rule in discovered_rules:
    print(f"Rule pattern has {len(rule.pattern.graph.nodes)} nodes")
    print(f"Rule has {len(rule.preconditions)} preconditions")
```

**Counterfactual Explanation**

```python
from CFE import CounterfactualExplainer, generate_counterfactual_explanation
from graph_matcher import Graph, Node, RxGNNs, Pattern

# Option 1: Quick function for simple use cases
attr_changes, edge_removals = generate_counterfactual_explanation(
    subgraph,           # Neighborhood subgraph of target vertex
    rxgnn_rule          # A rule from discovered_rules
)

# Option 2: More configurable explainer class
explainer = CounterfactualExplainer(
    threshold=5,        # Refinement threshold
    alpha=0.5,          # Distance decay factor
    lambda_factor=10.0  # Edge removal cost factor
)

# Generate explanations
attr_changes, edge_removals = explainer.explain(subgraph, rxgnn_rule)

# Interpret results
print(f"Attribute changes: {attr_changes}")
print(f"Edge removals: {edge_removals}")
```

Note that practical usage will require appropriate setup of graph structures, rule representation, and integration with your GNN model. The actual internal workings of these components involve complex mechanisms for rule discovery, pattern matching, and counterfactual optimization that are adaptable to various GNN architectures.

**Perplexity-Based Predicate Selection**

A key innovation in RxGNNs is its use of language models to prioritize logical predicates based on their explanatory quality. The `PredicateSelector` transforms logical predicates into natural language descriptions and assigns perplexity scores that reflect how coherent and plausible these explanations would seem to a human.

When the system encounters a predicate like `x0.debt_to_income ≥ 40%`, it transforms this into natural language. The language model then assesses how naturally this explanation fits with the overall pattern of negative predictions.

```python
def calculate_ppl(self, predicate, pattern):
    natural_language = self._predicate_to_natural_language(predicate, pattern)
    prompt = self.prompt_template.format(rule=natural_language)
    
    # LLM perplexity calculation
    encodings = self.tokenizer(prompt, return_tensors="pt").to(device)
    outputs = self.model(input_ids=encodings.input_ids, attention_mask=encodings.attention_mask)
    neg_log_likelihood = outputs.loss.item()
    
    return np.exp(neg_log_likelihood)
```

Lower perplexity scores indicate predicates that the language model finds more coherent and plausible for explaining negative predictions. These scores are used to prioritize predicates during rule discovery, ensuring that the resulting explanations align with human intuition about what constitutes a good explanation.

**Reinforcement Learning for Pattern Composition**

The pattern composition process uses a Deep Q-Network approach to learn optimal strategies for merging small motifs into informative patterns. This RL framework models composition as a sequential decision process where each state represents a pattern, actions correspond to merging specific vertices, and rewards are based on the resulting pattern's quality.

The DQN learns to select actions that maximize long-term rewards by balancing pattern discriminability (confidence) with support (frequency):

```python
def get_reward(self, old_pattern, new_pattern):
    old_confidence = self.matcher.get_pattern_confidence(old_pattern)
    new_confidence = self.matcher.get_pattern_confidence(new_pattern)
    
    confidence_change = new_confidence - old_confidence
    support_change = (new_support - old_support) / max(1, old_support)
    
    return confidence_change + 0.1 * support_change
```

The system employs standard RL techniques like experience replay and target networks to stabilize learning. During inference, the trained model selects the optimal vertex pairs to merge or decides to terminate the composition process. This approach allows the system to generate high-quality patterns without exhaustive enumeration, making it tractable for large graphs.

The state representation encodes pattern properties like node count, edge count, support, and confidence, while also incorporating GNN-derived features when available. This rich state representation enables the DQN to learn sophisticated merging strategies tailored to the specific GNN model being explained.

**Candidate Space Refinement for Counterfactual Explanations**

The counterfactual explanation component employs a sophisticated bidirectional constraint propagation approach to efficiently identify minimal graph perturbations. Rather than exhaustively testing all possible modifications, the system uses a dynamic programming approach that propagates constraints through a DAG-based representation of the pattern.

The refinement process alternates between forward and backward propagation passes until convergence:

```python
def _refine_candidate_space(self, cand_space, pattern_graph, dag, reverse_dag):
    refined_cand = copy.deepcopy(cand_space)
    
    for _ in range(self.max_iterations):
        refined_cand = self._dag_graph_dp(refined_cand, pattern_graph, dag)
        refined_cand = self._dag_graph_dp(refined_cand, pattern_graph, reverse_dag)
        
        # Check for convergence
        # ...
```

This bidirectional constraint propagation efficiently prunes the candidate space by identifying vertices that cannot participate in valid pattern matches. The resulting refined candidate space dramatically reduces the search space for counterfactual explanations.

# Acknowledgements

This project has benefited from the following open-source projects, to which we extend our gratitude:

- **torch_geometric**: https://github.com/pyg-team/pytorch_geometric
- **Mistral**: https://huggingface.co/mistralai
- **Qwen**: https://huggingface.co/Qwen
- **SimCSE**: https://github.com/princeton-nlp/SimCSE
- **vllm**: https://github.com/vllm-project/vllm
- **LLaMA-Factory**: https://github.com/hiyouga/LLaMA-Factory
- **GUNDAM**: https://github.com/MinovskySociety/GUNDAM

We acknowledge these contributions, which have significantly facilitated the development of our project.
